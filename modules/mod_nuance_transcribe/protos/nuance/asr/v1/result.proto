//  Copyright (C) 2020 - 2022 Nuance Communications, Inc. All Rights Reserved.
//
//  The copyright to the computer program(s) herein is the property of
//  Nuance Communications, Inc. The program(s) may be used and/or copied
//  only with the written permission from Nuance Communications, Inc.
//  or in accordance with the terms and conditions stipulated in the
//  agreement/contract under which the program(s) have been supplied.

syntax = "proto3";

package nuance.asr.v1;

import "nuance/rpc/error_details.proto"; // This is where LocalizedMessage is defined

option java_multiple_files = true;
option java_package = "com.nuance.rpc.asr.v1";
option java_outer_classname = "ResultProto";

/* 
Output message containing the result, including the result type, the start and end times, metadata about the job, and one or more recognition hypotheses. Included in RecognitionResponse.
*/
message Result {
  EnumResultType result_type = 1;            // Whether final, partial, or immutable results are returned. 
  uint32 abs_start_ms = 2;                   // Start time of the audio segment that generated this result. Offset, in milliseconds, from the beginning of the audio stream.
  uint32 abs_end_ms = 3;                     // End time of the audio segment that generated this result. Offset, in milliseconds, from the beginning of the audio stream.
  UtteranceInfo utterance_info = 4;          // Information about each sentence. 
  repeated Hypothesis hypotheses = 5;        // Repeated. One or more recognition variations. 
  DataPack data_pack = 6;                    // Data pack information.
  repeated Notification notifications = 7;   // List of notifications, if any.  
}

/* 
Input and output field specifying how results for each utterance are returned. In a request (RecognitionParameters), it specifies the desired result type. In a response (Result), it indicates the actual result type that was returned. 
*/
enum EnumResultType {
  FINAL = 0;              // Only the final version of each utterance is returned.
  PARTIAL = 1;            // Variable partial results are returned, followed by a final result.
  IMMUTABLE_PARTIAL = 2;  // Stabilized partial results are returned, followed by a final result.
}

/* 
Output message containing information about the recognized sentence in the result. Included in Result. 
*/
message UtteranceInfo {
  uint32 duration_ms = 1;               // Utterance duration in milliseconds. 
  uint32 clipping_duration_ms = 2;      // Milliseconds of clipping detected. 
  uint32 dropped_speech_packets = 3;    // Number of speech audio buffers discarded during processing. 
  uint32 dropped_nonspeech_packets = 4; // Number of non-speech audio buffers discarded during processing. 
  Dsp dsp = 5;                          // Digital signal processing results.
}

/*
Output message containing digital signal processing results. Included in UtteranceInfo.
*/
message Dsp {
  float snr_estimate_db = 1;          // The estimated speech-to-noise ratio.  
  float level = 2;                    // Estimated speech signal level.
  uint32 num_channels  = 3;           // Number of audio channels. Always 1, meaning mono.
  uint32 initial_silence_ms = 4;      // Milliseconds of silence observed before start of utterance.
  float initial_energy = 5;           // Energy feature value of first speech frame. 
  float final_energy = 6;             // Energy feature value of last speech frame. 
  float mean_energy = 7;              // Average energy feature value of utterance.
}

/* 
Output message containing one or more proposed transcripts of the audio stream. Included in Result. Each variation has its own confidence level along with the text in two levels of formatting. 
*/
message Hypothesis {
  oneof optional_hypothesis_confidence {
    float confidence = 1;               // The confidence score for the entire transcription, 0 to 1. 
  }
  oneof optional_hypothesis_average_confidence {
    float average_confidence = 2;       // The confidence score for the hypothesis, 0 to 1: the average of all word confidence scores based on their duration. 
  }
  bool rejected = 3;                    // Whether the hypothesis was rejected. 
  string formatted_text = 4;            // Formatted text of the result, e.g. $500. Actual formatting is controlled by formatting schemes and options. 
  string minimally_formatted_text = 5;  // Slightly formatted text of the result, e.g. Five hundred dollars. Words are spelled out, basic capitalization and punctuation are included.
  repeated Word words = 6;              // Repeated. Individual words in formatted_text with timestamps and confidence (if available).
  string encrypted_tokenization = 7;    // Nuance-internal representation of the recognition result. 
  oneof optional_hypothesis_grammar_id {
    string grammar_id = 9;              // Identifier of the matching grammar. Returned when result originates from SRGS grammar rather than generic dictation.
  }
  oneof optional_detected_wuw {
    string detected_wakeup_word = 10;   // The detected wakeup word when using a wakeup word resource.
  }
}

/* 
Output message containing one or more recognized words in the hypothesis, including the text, confidence score, and timing information. Included in Hypothesis. 
*/
message Word {
  string text = 1;                    // The recognized word. 
  oneof optional_word_confidence {
    float confidence = 2;             // The confidence score of the recognized word, 0 to 1.
  }
  uint32 start_ms = 3;                // Start time of the word. Offset, in milliseconds, from the beginning of the current audio segment (abs_start_ms).
  uint32 end_ms = 4;                  // End time of the word. Offset, in milliseconds, from the beginning of the current audio segment (abs_start_ms).
  uint32 silence_after_word_ms = 5;   // The amount of silence, in milliseconds, detected after the word. 
  oneof optional_word_grammar_rule {
    string grammar_rule = 6;          // The grammar rule that recognized the word text. Returned when result originates from SRGS grammar rather than generic dictation.
  }
}

/*
Output message containing information about the data pack used in the recognition. Included in Result. 
*/
message DataPack {
  string language = 1;                // Data pack language. 
  string topic = 2;                   // Data pack topic.
  string version = 3;                 // Data pack version.
  string id = 4;                      // Data pack identifier string, including nightly build information if applicable.
}

/*
Notification structure. Included in Result. 
*/

message Notification {
  int32 code = 1;                           // Notification unique code.
  EnumSeverityType severity = 2;            // Severity (see below).
  nuance.rpc.LocalizedMessage message = 3;  // Notification message.
  map<string, string> data = 4;             // Additional key, value pairs. 
}

/* 
Output field specifying a notification's severity. Included in Notification.
*/

enum EnumSeverityType {
  SEVERITY_UNKNOWN = 0;     
  SEVERITY_ERROR = 10;
  SEVERITY_WARNING = 20;
  SEVERITY_INFO = 30;
}